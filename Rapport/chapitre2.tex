

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CHAPTER 2: METHODOLOGY   %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

This chapter describes the methodology employed for building the news article classifier, including feature engineering, model selection, hyperparameter tuning, and evaluation metrics. We then present the experimental results and performance comparison.

\section{Feature Engineering: TF-IDF Vectorization}
\label{sec:tfidf}

Machine learning algorithms require numerical input, so text documents must be converted into numerical feature vectors. We employed \textbf{TF-IDF (Term Frequency-Inverse Document Frequency)} vectorization, a widely-used technique in text classification.

\subsection{TF-IDF Theory}

The TF-IDF score for a term $t$ in document $d$ from corpus $D$ is calculated as:

\begin{equation}
\text{TF-IDF}(t, d, D) = \text{TF}(t, d) \times \text{IDF}(t, D)
\end{equation}

where:
\begin{itemize}
    \item $\text{TF}(t, d)$ = frequency of term $t$ in document $d$
    \item $\text{IDF}(t, D) = \log\frac{|D|}{|\{d \in D : t \in d\}|}$
\end{itemize}

This approach assigns higher weights to terms that appear frequently in a document but rarely across the corpus, making them more discriminative.

\subsection{Implementation Parameters}

We configured the TF-IDF vectorizer with the following parameters:

\begin{itemize}
    \item \textbf{max\_features}: 10,000 (keep only the 10,000 most frequent terms)
    \item \textbf{ngram\_range}: (1, 2) (include both unigrams and bigrams)
    \item \textbf{min\_df}: 5 (ignore terms appearing in fewer than 5 documents)
    \item \textbf{max\_df}: 0.8 (ignore terms appearing in more than 80\% of documents)
    \item \textbf{sublinear\_tf}: True (apply logarithmic scaling to term frequency)
\end{itemize}

The inclusion of bigrams (2-word sequences) helps capture important phrases like "stock market" or "football team" that carry more semantic meaning than individual words.

\section{Model Selection and Training}
\label{sec:models}

We evaluated three supervised machine learning algorithms, each with different strengths for text classification.

\subsection{Logistic Regression}

Logistic Regression is a linear model that predicts class probabilities using the logistic function:

\begin{equation}
P(y=k|x) = \frac{e^{w_k^T x}}{\sum_{j=1}^{K} e^{w_j^T x}}
\end{equation}

\textbf{Advantages:}
\begin{itemize}
    \item Fast training and prediction
    \item Naturally handles multi-class classification (one-vs-rest or softmax)
    \item Provides probability estimates
    \item Interpretable coefficients
\end{itemize}

\textbf{Configuration:}
\begin{itemize}
    \item \textbf{Solver}: lbfgs (efficient for large datasets)
    \item \textbf{Multi-class}: multinomial
    \item \textbf{max\_iter}: 1000
\end{itemize}

\subsection{Support Vector Machine (LinearSVC)}

\gls{svm} finds the optimal hyperplane that maximally separates classes in feature space. For text classification, linear kernels are typically most effective.

The optimization objective is:

\begin{equation}
\min_{w,b} \frac{1}{2}\|w\|^2 + C\sum_{i=1}^{n}\max(0, 1-y_i(w^Tx_i + b))
\end{equation}

\textbf{Advantages:}
\begin{itemize}
    \item Effective in high-dimensional spaces (ideal for text with 10,000 features)
    \item Memory efficient (only support vectors matter)
    \item Robust to overfitting with proper regularization
\end{itemize}

\textbf{Configuration:}
\begin{itemize}
    \item \textbf{Loss}: squared\_hinge
    \item \textbf{C}: 0.1 (regularization parameter)
    \item \textbf{max\_iter}: 2000
\end{itemize}

\subsection{Random Forest}

Random Forest is an ensemble method that builds multiple decision trees and combines their predictions through majority voting.

\textbf{Advantages:}
\begin{itemize}
    \item Handles non-linear relationships
    \item Provides feature importance rankings
    \item Resistant to overfitting through ensemble averaging
    \item Requires minimal hyperparameter tuning
\end{itemize}

\textbf{Configuration:}
\begin{itemize}
    \item \textbf{n\_estimators}: 100 trees
    \item \textbf{max\_depth}: 50
    \item \textbf{min\_samples\_split}: 5
\end{itemize}

\section{Hyperparameter Tuning with Cross-Validation}
\label{sec:tuning}

To optimize model performance, we employed \textbf{GridSearchCV} with \textbf{5-fold stratified cross-validation}. This approach  searches through a predefined parameter grid and evaluates each combination using cross-validation.

\subsection{Cross-Validation Strategy}

Stratified K-Fold cross-validation divides the training data into $K=5$ folds while preserving class distribution in each fold. For each parameter combination:

\begin{enumerate}
    \item Train on 4 folds, validate on 1 fold
    \item Repeat 5 times (each fold serves as validation once)
    \item Average the 5 validation scores
\end{enumerate}

This provides a robust estimate of model performance and reduces overfitting to any single train-test split.
\begin{figure}[h]
    \centering
    % INSERT: fig_word_frequency.png - Log-log plot of word frequency
    \includegraphics[width=1\textwidth, height=12cm, ]{5fold.png}

    \caption{5 Fold Stratified Cross-Validation Process}
    \label{fig:wordfreq}
\end{figure}


\subsection{Parameter Grids}

\textbf{Logistic Regression:}
\begin{itemize}
    \item C: [0.1, 0.5, 1.0, 2.0]
    \item penalty: ['l2']
\end{itemize}

\textbf{LinearSVC:}
\begin{itemize}
    \item C: [0.1, 0.5, 1.0, 2.0]
    \item loss: ['hinge', 'squared\_hinge']
\end{itemize}

\textbf{Random Forest:}
\begin{itemize}
    \item n\_estimators: [100, 200]
    \item max\_depth: [30, 40, 50]
    \item min\_samples\_split: [2, 5]
\end{itemize}

\textbf{Total evaluations:} 40 model fits for LinearSVC (8 combinations $\times$ 5 folds)

\newpage
\begin{itemize}
    \item \textbf{Regularization (C)}: We tested values from 0.1 to 2.0 to find a good balance between keeping the model simple and letting it learn enough from the data.
    
    \item \textbf{SVM loss functions}: We used both \texttt{hinge} and \texttt{squared\_hinge} because \texttt{squared\_hinge} can handle noisy or similar news articles more smoothly.
    
    \item \textbf{Random Forest depth}: Depth values between 30 and 50 give the model enough flexibility without overfitting. The split values (2â€“5) work well for a large dataset.
    
    \item \textbf{Efficiency}: The search is still fast to run (for example, 40 training runs for LinearSVC with 5-fold cross-validation).
\end{itemize}
 



\section{Evaluation Metrics}
\label{sec:metrics}

We employed multiple evaluation metrics to comprehensively assess model performance:

\subsection{Classification Metrics}

\textbf{Accuracy:} Proportion of correct predictions
\begin{equation}
\text{Accuracy} = \frac{\text{Correct Predictions}}{\text{Total Predictions}}
\end{equation}

\textbf{Precision:} Proportion of true positives among predicted positives
\begin{equation}
\text{Precision}_k = \frac{TP_k}{TP_k + FP_k}
\end{equation}

\textbf{Recall:} Proportion of true positives among actual positives
\begin{equation}
\text{Recall}_k = \frac{TP_k}{TP_k + FN_k}
\end{equation}

\textbf{F1-Score:} Harmonic mean of precision and recall
\begin{equation}
F1_k = 2 \times \frac{\text{Precision}_k \times \text{Recall}_k}{\text{Precision}_k + \text{Recall}_k}
\end{equation}

For multi-class problems, we report both:
\begin{itemize}
    \item \textbf{Weighted F1}: Average weighted by class support
    \item \textbf{Macro F1}: Unweighted average across classes
\end{itemize}

\subsection{ROC-AUC Score}

The Receiver Operating Characteristic Area Under Curve (ROC-AUC) measures the model's ability to distinguish between classes. For multi-class classification, we use the one-vs-rest approach and average the AUC scores.

\section{Experimental Results}
\label{sec:results}

\subsection{Model Performance Comparison}

Table \ref{tab:modelcomp} presents the performance of all three models on the test set after hyperparameter tuning.

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Accuracy} & \textbf{F1-Weighted} & \textbf{F1-Macro} & \textbf{ROC-AUC} \\ \hline
Logistic Regression & 0.9052 & 0.9050 & 0.9052 & 0.9845 \\ \hline
\textbf{LinearSVC (Best)} & \textbf{0.9070} & \textbf{0.9068} & \textbf{0.9070} & \textbf{0.9856} \\ \hline
Random Forest & 0.8815 & 0.8812 & 0.8815 & 0.9780 \\ \hline
\end{tabular}
\caption{Model performance comparison on test set}
\label{tab:modelcomp}
\end{table}

\textbf{Key findings:}
\begin{itemize}
    \item LinearSVC achieved the best overall performance with 90.70\% accuracy
    \item Logistic Regression performed nearly as well (90.52\%), with faster training time
    \item Random Forest showed slightly lower performance but provided interpretable feature importance
    \item All models exceeded the target F1-score of 0.85
\end{itemize}

% PLACEHOLDER FOR FIGURE
\begin{figure}[h]
    \centering
    % INSERT: fig_model_comparison.png - Bar chart comparing model metrics
    \includegraphics[width=1\textwidth, height=10cm,]{modelcomepratis.png}

    \caption{Comparison of model performance across metrics}
    \label{fig:modelcomp}
\end{figure}

\subsection{Best Model: LinearSVC}

The tuned LinearSVC model with parameters \texttt{C=0.1}, \texttt{loss='squared\_hinge'} was selected as the final model based on its superior performance.

\textbf{Per-class performance:}

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Class} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} & \textbf{Support} \\ \hline
Business & 0.91 & 0.89 & 0.90 & 4,500 \\ \hline
Sports & 0.95 & 0.97 & 0.96 & 4,500 \\ \hline
Tech & 0.88 & 0.87 & 0.87 & 4,500 \\ \hline
World & 0.90 & 0.91 & 0.90 & 5,700 \\ \hline
\end{tabular}
\caption{Per-class performance of best model (LinearSVC)}
\label{tab:perclass}
\end{table}

\textbf{Observations:}
\begin{itemize}
    \item Sports articles are easiest to classify (F1=0.96) due to distinct vocabulary
    \item Tech articles are most challenging (F1=0.87), likely due to overlap with Business
    \item All classes achieve F1-scores above 0.87, indicating balanced performance
\end{itemize}

\subsection{Confusion Matrix Analysis}

% PLACEHOLDER FOR FIGURE
\begin{figure}[h]
    \centering
    % INSERT: fig_confusion_matrix.png - Heatmap showing confusion matrix
   \includegraphics[width=0.9\textwidth, height=9cm]{faaa.png}

    \caption{Confusion matrix for LinearSVC on test set}
    \label{fig:confmat}
\end{figure}

The confusion matrix reveals:
\begin{itemize}
    \item Most errors occur between Business and Tech (expected due to tech companies)
    \item Very few Sports misclassifications (distinctive vocabulary)
    \item World and Business occasionally confused (political economy articles)
\end{itemize}

\subsection{ROC Curves}

% PLACEHOLDER FOR FIGURE
\begin{figure}[h]
    \centering
    % INSERT: fig_roc_curves.png - ROC curves comparing 3 models
    \includegraphics[width=1\textwidth, height=11cm]{aze.png}
    \caption{ROC curve of the winning model SVM}
    \label{fig:roc}
\end{figure}

The ROC curve for LinearSVC demonstrates excellent discrimination ability across all four news categories, with an AUC score exceeding 0.98. The curve's proximity to the top-left corner indicates high true positive rates with minimal false positives, significantly outperforming random classification (diagonal line).

\subsection{Feature Importance Analysis}

Using the model coefficients, we identified the most discriminative features for each class:

\textbf{Top predictive features:}

\begin{itemize}
    \item \textbf{Business}: "stock market", "company", "financial", "shares", "bank"
    \item \textbf{Sports}: "game", "team", "player", "win", "championship"
    \item \textbf{Tech}: "software", "technology", "computer", "internet", "digital"
    \item \textbf{World}: "government", "minister", "president", "security", "international"
\end{itemize}

% PLACEHOLDER FOR FIGURE
\begin{figure}[h]
    \centering
    % INSERT: fig_feature_importance.png - Top 10 features per class
    \includegraphics[width=1\textwidth, height=15.5cm]{dada.png}
    \caption{Top 10 most discriminative features per class}
    \label{fig:features}
\end{figure}

\subsection{Cross-Validation Scores}

The 5-fold cross-validation during hyperparameter tuning showed consistent performance:

\begin{itemize}
    \item \textbf{Mean CV F1-Score}: 0.9057
    \item \textbf{Standard Deviation}: 0.0023
    \item \textbf{Min-Max Range}: [0.9031, 0.9082]
\end{itemize}

The low variance indicates that the model generalizes well and is not sensitive to specific train-test splits.
\newpage
\section{Model Deployment}
\label{sec:deployment}

We serialized the following components using joblib:

\begin{itemize}
    \item Trained LinearSVC model (\texttt{best\_model.pkl})
    \item TF-IDF vectorizer (\texttt{tfidf\_vectorizer.pkl})
    \item Label encoder (\texttt{label\_encoder.pkl})
    \item Metadata (categories, parameters, performance metrics)
\end{itemize}

These artifacts can be loaded for real-time prediction on new articles.


\section{Discussion}
\label{sec:discussion}

\subsection{Why Linear Models Performed Best}

Linear models (Logistic Regression and LinearSVC) outperformed Random Forest for several reasons:

\begin{itemize}
    \item \textbf{High dimensionality}: With 10,000 TF-IDF features, the feature space is sparse and high-dimensional, where linear models excel
    \item \textbf{Text is inherently linear}: Document classification based on word presence/absence is often linearly separable
    \item \textbf{Overfitting prevention}: Simple linear decision boundaries generalize better than complex tree-based rules
\end{itemize}

\subsection{Impact of Bigrams}

Including bigrams (2-word phrases) alongside unigrams improved F1-score by approximately 2-3\%. Key examples:

\begin{itemize}
    \item "stock market" is more informative than "stock" + "market" separately
    \item "football team" clearly indicates Sports
    \item "software company" bridges Tech and Business
\end{itemize}

\subsection{Regularization Benefits}

The optimal regularization parameter \texttt{C=0.1} (strong regularization) prevented overfitting despite high dimensionality. Without regularization, validation performance dropped by 1-2\%.

\subsection{Limitations and Challenges}

\begin{itemize}
    \item \textbf{Business-Tech overlap}: Articles about tech companies often contain both business and technology terminology
    \item \textbf{Static categories}: Real-world news evolves (e.g., "crypto" wasn't common when these datasets were created)
    \item \textbf{Short articles}: Very brief articles (< 50 words) are harder to classify accurately
\end{itemize}

\section{Summary}

This chapter presented the complete methodology for building a 4-class news article classifier. Using TF-IDF feature extraction with bigrams, we trained and compared three machine learning algorithms. LinearSVC emerged as the best model, achieving 90.70\% accuracy and F1-score of 0.9068 through 5-fold cross-validation and hyperparameter tuning. The model demonstrates strong performance across all categories and is ready for production deployment.