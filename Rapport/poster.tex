%==============================================================================
% SCIENTIFIC POSTER - NEWS CLASSIFICATION PROJECT
%==============================================================================

\documentclass[final,hyperref={pdfpagelabels=false}]{beamer}
\usepackage[orientation=portrait,size=a0,scale=1.4]{beamerposter}
\mode<presentation>{\usetheme{Berlin}}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath,amsthm,amssymb,latexsym}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{multicol}
\usepackage{tikz}

%==============================================================================
% COLORS
%==============================================================================
\definecolor{darkblue}{RGB}{0,51,102}
\definecolor{lightblue}{RGB}{102,178,255}
\setbeamercolor{block title}{fg=white,bg=darkblue}
\setbeamercolor{block body}{fg=black,bg=white}

%==============================================================================
% TITLE SECTION
%==============================================================================
\title{\huge Automated News Article Classification Using Machine Learning}
\author{Your Name}
\institute{Institut Supérieur des Technologies de Bizerte (ITBS)}
\date{\today}

%==============================================================================
% BEGIN DOCUMENT
%==============================================================================
\begin{document}
\begin{frame}[fragile]

%==============================================================================
% HEADER
%==============================================================================
\begin{block}{}
\centering
\textbf{\Huge Automated News Article Classification Using Machine Learning}\\[0.5cm]
\large Your Name | Institut Supérieur des Technologies de Bizerte (ITBS)
\end{block}

\vspace{0.5cm}

%==============================================================================
% MAIN CONTENT - 3 COLUMNS
%==============================================================================
\begin{columns}[t]

%==============================================================================
% LEFT COLUMN
%==============================================================================
\begin{column}{0.32\textwidth}

% INTRODUCTION
\begin{block}{1. Introduction \& Objectives}
\textbf{Goal:} Build an automated system to classify news articles into 4 categories

\vspace{0.3cm}
\textbf{Categories:}
\begin{itemize}
    \item Business
    \item Sports
    \item Technology
    \item World
\end{itemize}

\vspace{0.3cm}
\textbf{Why Important?}
\begin{itemize}
    \item Handles information overload
    \item Enables content personalization
    \item Automates news organization
\end{itemize}

\vspace{0.3cm}
\textbf{Target:} Achieve F1-score $\geq$ 0.85
\end{block}

\vspace{0.5cm}

% DATASET
\begin{block}{2. Dataset}
\textbf{Total: 144,522 articles from 2 sources}

\begin{center}
\begin{tabular}{lrr}
\toprule
\textbf{Source} & \textbf{Articles} & \textbf{\%} \\
\midrule
AG News & 127,600 & 88.3\% \\
20 Newsgroups & 16,922 & 11.7\% \\
\midrule
\textbf{Total} & \textbf{144,522} & \textbf{100\%} \\
\bottomrule
\end{tabular}
\end{center}

\vspace{0.3cm}
\textbf{Class Distribution (Balanced):}
\begin{center}
\begin{tabular}{lr}
\toprule
\textbf{Category} & \textbf{Count} \\
\midrule
Business & 32,821 (22.7\%) \\
Sports & 35,603 (24.6\%) \\
Technology & 39,381 (27.2\%) \\
World & 36,717 (25.4\%) \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Split:} 70\% Train | 15\% Val | 15\% Test
\end{block}

\vspace{0.5cm}

% PREPROCESSING
\begin{block}{3. Text Preprocessing}
\begin{enumerate}
    \item \textbf{Cleaning:} Remove URLs, HTML, special chars
    \item \textbf{Normalization:} Lowercase conversion
    \item \textbf{Stopword Removal:} Filter common words
    \item \textbf{Quality Check:} Remove short texts (<20 chars)
\end{enumerate}
\end{block}

\end{column}

%==============================================================================
% MIDDLE COLUMN
%==============================================================================
\begin{column}{0.32\textwidth}

% METHODOLOGY
\begin{block}{4. Feature Extraction: TF-IDF}
\textbf{Term Frequency-Inverse Document Frequency}

$$\text{TF-IDF}(t,d) = \text{TF}(t,d) \times \text{IDF}(t)$$

\vspace{0.3cm}
\textbf{Configuration:}
\begin{itemize}
    \item Max features: 10,000
    \item N-grams: unigrams + bigrams (1,2)
    \item Min document frequency: 5
    \item Max document frequency: 80\%
\end{itemize}
\end{block}

\vspace{0.5cm}

% MODELS
\begin{block}{5. Machine Learning Models}

\textbf{Three Algorithms Compared:}

\vspace{0.3cm}
\textbf{1. Logistic Regression}
\begin{itemize}
    \item Linear probabilistic classifier
    \item Parameter: C $\in$ \{0.1, 0.5, 1.0, 2.0\}
\end{itemize}

\vspace{0.3cm}
\textbf{2. LinearSVC (Support Vector Machine)}
\begin{itemize}
    \item Linear decision boundary
    \item Parameters: C, loss function
    \item \textcolor{red}{\textbf{BEST MODEL}}
\end{itemize}

\vspace{0.3cm}
\textbf{3. Random Forest}
\begin{itemize}
    \item Ensemble of decision trees
    \item Parameters: n\_estimators, max\_depth
\end{itemize}

\vspace{0.3cm}
\textbf{Hyperparameter Tuning:}
\begin{itemize}
    \item 5-fold stratified cross-validation
    \item Grid search optimization
    \item Scoring: F1-weighted
\end{itemize}
\end{block}

\vspace{0.5cm}

% EVALUATION METRICS
\begin{block}{6. Evaluation Metrics}
$$\text{Accuracy} = \frac{\text{Correct Predictions}}{\text{Total Predictions}}$$

$$\text{Precision} = \frac{TP}{TP + FP}$$

$$\text{Recall} = \frac{TP}{TP + FN}$$

$$\text{F1-Score} = 2 \times \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}$$
\end{block}

\end{column}

%==============================================================================
% RIGHT COLUMN
%==============================================================================
\begin{column}{0.32\textwidth}

% RESULTS
\begin{block}{7. Results}

\textbf{Model Performance Comparison:}

\begin{center}
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Acc} & \textbf{Prec} & \textbf{Rec} & \textbf{F1} \\
\midrule
LogReg & 0.902 & 0.902 & 0.902 & 0.902 \\
\textcolor{red}{\textbf{LinearSVC}} & \textcolor{red}{\textbf{0.908}} & \textcolor{red}{\textbf{0.908}} & \textcolor{red}{\textbf{0.908}} & \textcolor{red}{\textbf{0.907}} \\
RandForest & 0.798 & 0.820 & 0.798 & 0.800 \\
\bottomrule
\end{tabular}
\end{center}

\vspace{0.5cm}

\textbf{Best Model: LinearSVC (C=0.1, squared hinge)}
\begin{itemize}
    \item \textbf{Accuracy: 90.70\%}
    \item \textbf{F1-Score: 0.9068}
    \item Target achieved: F1 $>$ 0.85 ✓
\end{itemize}

\vspace{0.5cm}

\textbf{Per-Class Performance:}
\begin{center}
\begin{tabular}{lccc}
\toprule
\textbf{Category} & \textbf{Prec} & \textbf{Rec} & \textbf{F1} \\
\midrule
Business & 0.89 & 0.87 & 0.88 \\
Sports & 0.95 & 0.97 & 0.96 \\
Technology & 0.89 & 0.90 & 0.89 \\
World & 0.91 & 0.90 & 0.91 \\
\bottomrule
\end{tabular}
\end{center}

\textbf{Key Findings:}
\begin{itemize}
    \item \textbf{Best:} Sports (F1=0.96) - distinctive vocabulary
    \item \textbf{Challenging:} Business-Tech overlap
    \item \textbf{Balanced:} All classes F1 $>$ 0.87
\end{itemize}

\end{block}

\vspace{0.5cm}

% CONCLUSION
\begin{block}{8. Conclusion \& Future Work}

\textbf{Achievements:}
\begin{itemize}
    \item Developed high-accuracy classifier (90.7\%)
    \item Exceeded target performance (F1 = 0.907 $>$ 0.85)
    \item Balanced performance across all categories
\end{itemize}

\vspace{0.3cm}
\textbf{Future Improvements:}
\begin{itemize}
    \item Deep learning (BERT, transformers)
    \item Multi-label classification
    \item Real-time deployment
    \item Expand to more categories
\end{itemize}
\end{block}

\vspace{0.5cm}

% REFERENCES
\begin{block}{9. Key References}
\small
\begin{itemize}
    \item Pedregosa et al. (2011). Scikit-learn: ML in Python
    \item Salton \& Buckley (1988). TF-IDF weighting
    \item Cortes \& Vapnik (1995). Support Vector Machines
    \item Breiman (2001). Random Forests
\end{itemize}
\end{block}

\end{column}

\end{columns}

\end{frame}
\end{document}
