%% @Author: Ahmad Ben Taleb
%  @Date:   2026
%% @Class:  PFE ITBS V3.

\documentclass[a4paper, oneside, 12pt, final]{extreport}
\usepackage{graphicx}

\parindent 0cm
\usepackage{makeidx}
\usepackage{tcolorbox}
\makeindex
%\usepackage[french]{babel}
\usepackage[lined,boxed,commentsnumbered, english, ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{definition}{Definition}[chapter]
\newtheorem{exemple}{Example}[chapter]


%\usepackage[nottoc]{tocbibind}
%\addcontentsline{toc}{section}{References}

\providecommand{\keywords}[1]{\textbf{\textit{Mots clés---}} #1}
\providecommand{\keywordss}[1]{\textbf{\textit{Keywords---}} #1}

\usepackage{etoolbox}
%\makeatletter
%\patchcmd{\thebibliography}{%
%  \chapter*{\bibname}\@mkboth{\MakeUppercase\bibname}%{\MakeUppercase\bibname}}{%
%  \section{References}}{}{}
%\makeatother



\usepackage[nottoc]{tocbibind}

\textwidth 18cm
\textheight 24cm
\topmargin -0.5cm
\oddsidemargin -1cm

% set font encoding for PDFLaTeX or XeLaTeX
\usepackage{ifxetex}
\ifxetex
  \usepackage{fontspec}
\else
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{lmodern}
\fi


% Enable SageTeX to run SageMath code right inside this LaTeX file.
% documentation: http://mirrors.ctan.org/macros/latex/contrib/sagetex/sagetexpackage.pdf
%\usepackage{sagetex}


\newcommand{\reportTitle} {%
  %\textsc{Graduation Project Report}
  \textsc{Mini Projet Machine Learning}
}

\newcommand{\reportAuthor} {%
  Gharbi  \textsc{Mahdi}%
\\ \\ \\ \\
  Gharbi  \textsc{Skandar}%
}


  

\newcommand{\reportSubject} {%
    Articles Classification  Using
    \\ \\ Machine Learning%
}



\newcommand{\studyDepartment} {%
  Entreprise d'accueil 
  
}

\newcommand{\ITBS} {%
  %IT Business School
\\ Ecole Supérieure Privée des Technologies de l'Information et de Management de Nabeul
}

%\newcommand{\codePFE} {% Reference
%  Code PFE%
%}


\newcommand{\AU} {
\centering \textbf{Année Universitaire 2025-2026}
}


\newcommand{\specialcell}[1]{%
  \begin{tabularx}{\textwidth}{@{}X@{}}#1\end{tabularx}%
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Add your own commands here
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\MyCommand} {%
  Does nothing really%
}


% used in maketitle
\title{\reportSubject}
\author{\reportAuthor}

% Enable SageTeX to run SageMath code right inside this LaTeX file.
% documentation: http://mirrors.ctan.org/macros/latex/contrib/sagetex/sagetexpackage.pdf
%\usepackage{sagetex}

%\hypersetup{
%  pdftitle={\reportTitle~-~\reportSubject},%
%  pdfauthor={\reportAuthor},%
%  pdfsubject={\reportSubject},%
%  pdfkeywords={report} {internship} {pfe} {enis}
%}

\usepackage{graphics}
\usepackage{graphicx}


\usepackage[acronym,toc,section=chapter]{glossaries}
\makeglossaries

\newacronym{abc}{ABC}{A contrived acronym}
\newacronym{efg}{EFG}{Another acronym}
\newacronym{svm}{SVM}{Support Vector Machines}

\pagenumbering{roman} 

\usepackage[utf8]{inputenc}
%\usepackage[french]{babel}

\begin{document}
\thispagestyle{empty}
\begin{titlepage}
\begin{center}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THE HEADER
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\includegraphics[scale=0.20]{embleme.jpg}
\vspace{0.5cm}

{%
  \fontsize{9pt}{9pt}\selectfont%
  \begin{tabular}{c}
    R\'epublique Tunisienne \\
    Minist\`ere de l'Enseignement Supérieur et de la Recherche Scientifique   \ITBS{}\\ 
  \end{tabular}
}

\vspace{1cm}

\includegraphics[scale=0.1]{logonoir.png}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THE PAGE CONTENT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\vspace{10pt} {%
  \renewcommand*{\familydefault}{\defaultFont}
  \fontsize{46pt}{46pt}\selectfont%
  % MEMOIRE\\%
  %\reportTitle{}%\\\textsc{Report}\\%
}

\vspace{5pt}

\vspace{15pt}
{\textit{Rapport de Mini projet Machine learning}}\\

\vspace{10pt}
{\textbf{\large Diplôme National d'Ingénieur en Génie Informatique \\ Spécialité Génie Logiciel}}\\

%\includegraphics[scale=0.1]{logonoir.png}\\
\vspace{5pt}
\textbf{\textit{Réalisé par}}\\
\vspace{10pt} {%
  \fontsize{14pt}{14pt}\selectfont%
  {\bfseries\Large\sc \reportAuthor}\\
}%

\vspace{5pt} {%
  \renewcommand*{\familydefault}{\defaultFont}
  \fontsize{27pt}{27pt}\selectfont%
  \rule{0.5\textwidth}{.4pt}\\
  \vspace{7pt}
  \reportSubject{}\\%
  \vspace{7pt}
  \rule{0.5\textwidth}{.4pt}
}

\vspace{5pt}

%Soutenu le \dateSoutenance, devant la commission d'examen:\\
\vspace{10pt}

\begin{table}[h]
\begin{tabular}{lcr}
\textbf{Encadrant Académique:} M Ahmad Ben Taleb 
\end{tabular}
\end{table}




%\vfill

\vspace{40pt}



\centering
\includegraphics[scale=0.08]{logonoir.png}
\end{center}
\vspace{40pt}
\AU\\
\end{titlepage}

% ###############################
% # HELP COMMANDS               #
% ###############################
%
% -1 \part{part}
%  0 \chapter{chapter}
%  1 \section{section}
%  2 \subsection{subsection}
%  3 \subsubsection{subsubsection}
%  4 \paragraph{paragraph}
%  5 \subparagraph{subparagraph}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Dédicace et Remerciements
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{Accord_depot}

%\chapter*{D\'edicace}
%\addcontentsline{toc}{chapter}{Dedication}

%
%\nopagebreak{%
% And maybe a quote here
% \raggedright\hspace{5.75cm} To all of you,~\\
%\raggedright\hspace{7.75cm} I dedicate this work.
%  \raggedleft\normalfont\large\itshape{} \reportAuthor\par%
%}
%
%\cleardoublepage%


%\chapter*{Remerciements}
%\addcontentsline{toc}{chapter}{Thanks}



\input{abstract.tex}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Divers chapitres
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\addtocontents{toc}{\protect\setcounter{tocdepth}{4}}
\tableofcontents
%\addcontentsline{toc}{chapter}{\contentsname}

\listoffigures
%\addcontentsline{toc}{chapter}{Liste des Figures}
\listoftables
%\addcontentsline{toc}{chapter}{Liste des Tableaux}
\printglossaries

\cleardoublepage

\newpage
\pagenumbering{arabic}
\chapter*{Introduction}
\label{chap:general_intorduction}
\input{introduction.tex}


\chapter{Data studied}%
\label{chap:chapterone}
\input{chapitre1.tex}

\chapter{Models Used and Applications}
\label{chap:2}
\input{chapitre2.tex}


\chapter*{Conclusion et Perspectives}
\label{chap:conclusion}
\markboth{\MakeUppercase{Conclusion et Perspectives}}{}%
\addcontentsline{toc}{chapter}{Conclusion et Perspectives}

\section*{Summary of Achievements}

This project successfully developed a production-ready machine learning system for automatic news article classification. We addressed the challenge of categorizing textual documents into four well-defined categories: Business, Sports, Technology, and World news.

Our main contributions include:

\begin{itemize}
    \item \textbf{Robust Dataset}: Compiled and preprocessed ~128,000 news articles from two established sources (AG News and 20 Newsgroups), ensuring balanced class distribution
    \item \textbf{Comprehensive Pipeline}: Implemented a complete text preprocessing pipeline including cleaning, normalization, and quality assessment
    \item \textbf{Feature Engineering}: Applied TF-IDF vectorization with bigrams, capturing both individual terms and meaningful phrases
    \item \textbf{Model Comparison}: Systematically evaluated three algorithms (Logistic Regression, LinearSVC, Random Forest) with rigorous cross-validation
    \item \textbf{Strong Performance}: Achieved 90.70\% accuracy with LinearSVC, exceeding the target F1-score of 0.85 across all classes
    \item \textbf{Deployment-Ready}: Serialized trained models and vectorizers for production use
\end{itemize}

\section*{Key Findings}

Several important insights emerged from this work:

\begin{enumerate}
    \item \textbf{Linear models excel for text classification}: Despite their simplicity, Logistic Regression and LinearSVC outperformed Random Forest in the high-dimensional TF-IDF feature space
    
    \item \textbf{Bigrams improve performance}: Including 2-word phrases alongside individual words increased F1-score by 2-3\%, capturing semantic units like "stock market" and "football team"
    
    \item \textbf{Regularization is crucial}: Strong regularization (C=0.1) prevented overfitting despite 10,000 features, demonstrating that simpler models generalize better
    
    \item \textbf{Class-specific challenges}: Sports articles were easiest to classify (F1=0.96) due to distinctive vocabulary, while Tech-Business overlap posed the greatest challenge (F1=0.87)
    
    \item \textbf{Cross-validation reliability}: Low variance in 5-fold CV scores (std=0.0023) confirmed that the model is robust and not dependent on specific data splits
\end{enumerate}

\section*{Practical Applications}

The developed classifier has immediate real-world applications:

\begin{itemize}
    \item \textbf{Content Management Systems}: Automatically tag and organize incoming news articles
    \item \textbf{Personalized Recommendations}: Route articles to users based on their interests
    \item \textbf{News Aggregation}: Filter and categorize content from multiple sources
    \item \textbf{Information Retrieval}: Improve search by category-specific indexing
    \item \textbf{Content Moderation}: Flag misclassified or ambiguous articles for human review
\end{itemize}

\section*{Limitations}

Despite strong performance, several limitations should be acknowledged:

\begin{itemize}
    \item \textbf{Category Overlap}: Articles about technology companies often blend Business and Tech themes, causing occasional misclassifications
    
    \item \textbf{Temporal Drift}: News vocabulary evolves (e.g., "cryptocurrency", "AI", "pandemic"), requiring periodic model retraining
    
    \item \textbf{Short Documents}: Articles with fewer than 50 words provide insufficient context for accurate classification
    
    \item \textbf{Domain Specificity}: The model is trained on general news and may not generalize well to specialized domains (medical, legal, scientific)
    
    \item \textbf{Multilingual Support}: Currently limited to English; extending to other languages requires new training data
\end{itemize}

\section*{Future Work and Perspectives}

Several promising directions for future research and development:

\subsection*{Deep Learning Approaches}

\begin{itemize}
    \item \textbf{Transformer Models}: Implement BERT, RoBERTa, or DistilBERT for contextualized word embeddings that capture semantic nuances better than TF-IDF
    
    \item \textbf{Transfer Learning}: Fine-tune pre-trained language models on news-specific corpora for improved performance with less training data
    
    \item \textbf{Multi-label Classification}: Extend the system to assign multiple categories when articles span several topics
\end{itemize}

\subsection*{Enhanced Features}

\begin{itemize}
    \item \textbf{Named Entity Recognition}: Extract and leverage entities (persons, organizations, locations) as additional features
    
    \item \textbf{Topic Modeling}: Incorporate latent topic distributions (LDA, NMF) as supplementary features
    
    \item \textbf{Sentiment Analysis}: Add sentiment scores to distinguish opinion pieces from factual reporting
    
    \item \textbf{Metadata Integration}: Include publication date, source, and author information
\end{itemize}

\subsection*{Model Improvements}

\begin{itemize}
    \item \textbf{Active Learning}: Implement uncertainty sampling to identify difficult cases for human annotation, improving model with minimal labeling effort
    
    \item \textbf{Online Learning}: Enable incremental model updates as new articles arrive, adapting to vocabulary drift
    
    \item \textbf{Ensemble Methods}: Combine predictions from multiple models (stacking) to improve robustness
    
    \item \textbf{Confidence Calibration}: Better calibrate probability outputs for risk-aware decision making
\end{itemize}

\subsection*{System Extensions}

\begin{itemize}
    \item \textbf{Real-time API}: Deploy as a REST API service with sub-second latency for production integration
    
    \item \textbf{Explainability Dashboard}: Develop a web interface showing which words/phrases influenced each prediction
    
    \item \textbf{A/B Testing Framework}: Continuously evaluate model variants in production
    
    \item \textbf{Multilingual Support}: Extend to French, Arabic, and other languages relevant for international news
\end{itemize}

\subsection*{Research Directions}

\begin{itemize}
    \item \textbf{Zero-shot Classification}: Investigate methods to classify articles into unseen categories without retraining
    
    \item \textbf{Few-shot Learning}: Enable rapid adaptation to new categories with minimal examples
    
    \item \textbf{Adversarial Robustness}: Test model resilience against intentionally misleading text or adversarial attacks
    
    \item \textbf{Bias Detection}: Analyze and mitigate potential biases in classification across different news sources
\end{itemize}

\section*{Concluding Remarks}

In this project, we showed that classic machine learning models can perform very well on text classification when they are used correctly. By cleaning the data, choosing good features, tuning the models with cross-validation, and carefully evaluating the results, we were able to build a classifier that reaches over 90\% accuracy.


The system is also easy to improve in the future. New features, different algorithms, or even deep-learning models can be added without changing the whole structure. The trained models are ready to be used in real applications, and the reported limitations give a clear idea of what can be improved next.

Overall, this work helps make news articles easier to organize and understand using automatic classification. This can be useful for both content creators and readers, especially today when information is growing faster than ever.
\newpage
\appendix
\addcontentsline{toc}{chapter}{Appendices}

\chapter{Python Code and Implementation}
\label{chap:appendix}


\section{Data Preprocessing Pipeline}

\begin{verbatim}
import re
import nltk
from nltk.corpus import stopwords

def clean_text(text):
    """Clean and preprocess text data"""
    # Lowercase
    text = text.lower()
    
    # Remove URLs
    text = re.sub(r'http\S+|www\S+', '', text)
    
    # Remove HTML tags
    text = re.sub(r'<.*?>', '', text)
    
    # Remove special characters and digits
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    
    # Remove extra whitespace
    text = ' '.join(text.split())
    
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    words = text.split()
    text = ' '.join([w for w in words if w not in stop_words])
    
    return text

# Apply preprocessing
df['text_clean'] = df['text'].apply(clean_text)
\end{verbatim}


\section{TF-IDF Feature Extraction}

\begin{verbatim}
from sklearn.feature_extraction.text import TfidfVectorizer

# Initialize TF-IDF vectorizer with bigrams
vectorizer = TfidfVectorizer(
    max_features=10000,
    ngram_range=(1, 2),
    min_df=5,
    max_df=0.8,
    sublinear_tf=True
)

# Fit and transform training data
X_train = vectorizer.fit_transform(train_df['text_clean'])
X_test = vectorizer.transform(test_df['text_clean'])
\end{verbatim}


\section{Model Training}

\begin{verbatim}
from sklearn.svm import LinearSVC
from sklearn.model_selection import GridSearchCV, StratifiedKFold

# Define model
model = LinearSVC(max_iter=2000, random_state=42, dual=False)

# Hyperparameter grid
param_grid = {
    'C': [0.1, 0.5, 1.0, 2.0],
    'loss': ['hinge', 'squared_hinge']
}

# Grid search with cross-validation
grid_search = GridSearchCV(
    model,
    param_grid,
    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),
    scoring='f1_weighted',
    n_jobs=-1
)

# Train model
grid_search.fit(X_train, y_train)

# Best model
best_model = grid_search.best_estimator_
print(f"Best parameters: {grid_search.best_params_}")
\end{verbatim}


\section{Model Evaluation}

\begin{verbatim}
from sklearn.metrics import classification_report, confusion_matrix

# Predictions
y_pred = best_model.predict(X_test)

# Classification report
print(classification_report(y_test, y_pred, 
                          target_names=categories))

# Confusion matrix
cm = confusion_matrix(y_test, y_pred)
\end{verbatim}
\section{Libraries used}
 
  \begin{table}[h]
\centering
\begin{tabular}{|l|l|p{7cm}|}
\hline
\textbf{Library} & \textbf{Version} & \textbf{Purpose} \\ \hline
NumPy & 1.24+ & Numerical computing and array operations \\ \hline
Pandas & 2.0+ & Data manipulation and DataFrame handling \\ \hline
Scikit-learn & 1.3+ & Machine learning algorithms, TF-IDF, cross-validation, metrics \\ \hline
Matplotlib & 3.7+ & Data visualization and plotting \\ \hline
Seaborn & 0.12+ & Statistical visualizations and heatmaps \\ \hline
NLTK & 3.8+ & Natural language preprocessing (tokenization, stopwords) \\ \hline
Joblib & 1.3+ & Model serialization and persistence \\ \hline
\end{tabular}
\caption{Python libraries used in the project}
\label{tab:libraries}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Don't touch this, it is auto generated
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\nocite{*}

%\phantomsection{}
%\addcontentsline{toc}{chapter}{Webography}
%\printbibliography[title={Webography},type=online]

%\phantomsection{}
%\addcontentsline{toc}{chapter}{Bibliography}
%\printbibliography[title={Bibliography},nottype=online]

%\printbibheading %exemple de bibliographie divisée en sections. Pour ajouter des oeuvres non citées,utiliser \nocite

%\printbibliography[keyword=pratique,heading=subbibliography,title={Théories littéraires dans les jeux vidéo}]
%\printbibliography[keyword=litteraire,heading=subbibliography,title={Narratologie et structuralisme}]

%\printbibliography[keyword=jeu,heading=subbibliography,title={\emph{Games studies}}]

\bibliographystyle{apalike}
%\bibliographystyle{plain}
\renewcommand{\bibname}{References}
\bibliography{Biblio.bib}

\cleardoublepage%


\printindex


\input{abstract}


\end{document}
